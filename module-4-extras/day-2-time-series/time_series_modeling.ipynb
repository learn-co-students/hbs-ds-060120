{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from random import gauss as gs\n",
    "import datetime\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from matplotlib.pylab import rcParams\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## White Noise\n",
    "\n",
    "The idea behind white noise is that it is truly random.\n",
    "\n",
    "We don't want white noise to describe our model per se, but we *do* want it to describe our model *error*.\n",
    "\n",
    "Can you explain these truisms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make some white noise!\n",
    "\n",
    "rands = []\n",
    "for _ in range(1000):\n",
    "    rands.append(gs(0, 1))\n",
    "series = pd.Series(rands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(-10, 10, 1000)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(X, series);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we do a seasonal decomposition on this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series.index[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_index = pd.to_datetime(series.index, unit='D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_index[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series.index = time_index\n",
    "decomp_white = sm.tsa.seasonal_decompose(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "plt.subplot(411)\n",
    "plt.plot(series.index, decomp_white.observed, label='Original')\n",
    "plt.legend()\n",
    "plt.subplot(412)\n",
    "plt.plot(series.index, decomp_white.trend, label='Trend')\n",
    "plt.legend()\n",
    "plt.subplot(413)\n",
    "plt.plot(series.index, decomp_white.seasonal, label='Seasonal')\n",
    "plt.legend()\n",
    "plt.subplot(414)\n",
    "plt.plot(series.index, decomp_white.resid, label='Residual')\n",
    "plt.legend()\n",
    "plt.subplots_adjust(hspace=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Football Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb = pd.read_csv('data/google-trends_football_us.csv').iloc[1:, :]\n",
    "fb.columns = ['counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb['counts'] = fb['counts'].replace('<1', '0')\n",
    "fb['counts'] = fb['counts'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series as Both Predictor and Target?\n",
    "\n",
    "Often, the phenomenon we want to capture with a time series is a dataset being correlated with *itself*.\n",
    "\n",
    "Well, of course every dataset is perfectly correlated with itself. But what we're after now is the idea that a series is correlated with *earlier versions* of itself.\n",
    "\n",
    "Consider the problem of trying to predict tomorrow's closing price for some stock on the market. One may consider lots of features, like what sort of company it is to which the stock belongs or whether that company has been in the news recently.\n",
    "\n",
    "But it is very often the case that one of the most helpful predictors of tomorrow's price is *today's* price. And so we want to build a model where one of our predictors is an earlier version of our target.\n",
    "\n",
    "One tool we can use is **`df.rolling()`**, which creates a Rolling object that we can use to calculate statistics dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(fb.rolling(window=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb.rolling(window=3).mean().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb['roll_avg'] = fb.rolling(window=2).mean()\n",
    "\n",
    "fb.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb['roll_avg'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(fb.index[:30], fb['counts'][:30])\n",
    "plt.scatter(fb.index[1:31], fb['roll_avg'][1:31]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "\n",
    "lr.fit(fb[['roll_avg']][1:], fb['counts'][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 4))\n",
    "plt.plot(fb.index[1:25], fb['counts'][1:25], label='Data')\n",
    "plt.plot(fb.index[1:25], lr.predict(fb[['roll_avg']][1:25]),\n",
    "         label='Predicted')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autocorrelation and Partial Autocorrelation Functions\n",
    "\n",
    "Pandas and statsmodels offer autocorrelation (ACF) and partial autocorrelation (PACF) plotting tools. The idea here is to look at the correlation of a series with itself for some particular interval or *lag*. The key difference between the full and the partial autocorrelation functions is that the partial autocorrelation function ignores intervening intervals. For more, see [this post](https://machinelearningmastery.com/gentle-introduction-autocorrelation-partial-autocorrelation/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autocorrelation\n",
    "\n",
    "The basic idea of autocorrelation is simple: See how a series correlates with a \"lagged\" version of itself. If my sequence is $S_0 = (x_0, x_1, x_2, ... , x_n)$, then I can measure the Pearson correlation between the first $n-k + 1$ terms of $S_0$ and $S_{lag} = (x_k, x_{k+1}, x_{k+2}, ... , x_n)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acf(fb['counts'], nlags=20, fft=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot using `.autocorr()`\n",
    "\n",
    "X = np.arange(0, 100, 1)\n",
    "Y = np.zeros(100)\n",
    "for j in X[1:]:\n",
    "    Y[j] = fb['counts'].autocorr(lag=j)\n",
    "plt.figure(figsize=(20, 4))\n",
    "plt.plot(X, Y)\n",
    "plt.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To construct the autocorrelation function, we take the covariance of our time series with a lagged version\n",
    "# and then divide by the variance of the series.\n",
    "\n",
    "X = np.arange(0, 100, 1)\n",
    "Y = np.zeros(100)\n",
    "for j in X[1:]:\n",
    "    Y[j] += np.cov(fb['counts'][:-j], fb['counts'][j:])[0, 1] / np.var(fb['counts']) * ((180-j) / 180)\n",
    "plt.figure(figsize=(20, 4))\n",
    "plt.plot(X, Y)\n",
    "plt.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 4))\n",
    "pd.plotting.autocorrelation_plot(fb['counts']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The horizontal bands represent condfidence intervals, which are calculated by taking relevant z-scores of the standard normal distribution and dividing by the square root of the number of observations. For more, see [here](https://www.itl.nist.gov/div898/handbook/eda/section3/autocopl.htm).\n",
    "\n",
    "Calculation of the 95%-confidence interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.norm().ppf(0.975) / np.sqrt(fb['counts'].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculation of the 99%-confidence interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.norm().ppf(0.995) / np.sqrt(fb['counts'].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the `plot_acf()` function from statsmodels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 20, 4\n",
    "\n",
    "plot_acf(fb['counts'], lags=125, alpha=None);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partial Autocorrelation\n",
    "\n",
    "The idea behind partial Autocorrelation is to compare a series to a lagged version of itself while abstracting away from intermediate values. In effect, this amounts to exploring the correlations among *residuals*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pacf(fb['counts'], nlags=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculation #1\n",
    "\n",
    "One way of computing the partial autocorrelation is by fitting regressions to residuals from a simple dummy model that always predicts the mean. The coefficient of the final term will be the partial autocorrelation for the corresponding number of lags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tilde = fb['counts'] - fb['counts'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = (fb['counts'][:-1] - fb['counts'].mean()).values.reshape(-1, 1)\n",
    "x_2 = (fb['counts'][:-2] - fb['counts'].mean()).values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "\n",
    "lr.fit(np.concatenate([x_1[1:], x_2], axis=1), y_tilde[2:]).coef_[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = (fb['counts'][:-1] - fb['counts'].mean()).values.reshape(-1, 1)\n",
    "x_2 = (fb['counts'][:-2] - fb['counts'].mean()).values.reshape(-1, 1)\n",
    "x_3 = (fb['counts'][:-3] - fb['counts'].mean()).values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr2 = LinearRegression()\n",
    "\n",
    "lr2.fit(np.concatenate([x_1[2:], x_2[1:], x_3], axis=1), y_tilde[3:]).coef_[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculation #2\n",
    "\n",
    "An alternative way of calculating these values is to solve the matrix equation:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "\\rho(0) & ... & \\rho(k-1) \\\\\n",
    "... & ... & ... \\\\\n",
    "\\rho(k-1) & ... & \\rho(0)\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\phi_{k1} \\\\\n",
    "... \\\\\n",
    "\\phi_{kk}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\rho(1) \\\\\n",
    "... \\\\\n",
    "\\rho(k)\n",
    "\\end{bmatrix}$,\n",
    "\n",
    "where $\\rho(k)$ is the autocorrelation for $k$ lags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row1 = acf(fb['counts'], nlags=1, fft=False)\n",
    "row2 = acf(fb['counts'], nlags=1, fft=False)[::-1]\n",
    "\n",
    "autos = np.vstack([row1, row2])\n",
    "autos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = acf(fb['counts'], nlags=2, fft=False)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.solve(autos, b)[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more on this method, see [this post](https://stats.stackexchange.com/questions/129052/acf-and-pacf-formula)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 20, 4\n",
    "\n",
    "plot_pacf(fb['counts'], lags=20, alpha=0.05);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more on ACF and PACF, see [these slides](http://people.cs.pitt.edu/~milos/courses/cs3750/lectures/class16.pdf) and [this post](https://towardsdatascience.com/significance-of-acf-and-pacf-plots-in-time-series-analysis-2fa11a5d10a8)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARMA Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'AR' is for \"**Auto-Regressive**\": The prediction for today will be a function of the value for previous days.\n",
    "\n",
    "    The number of lag periods we want to include will be a parameter in the statsmodels model object (\"p\").\n",
    "\n",
    "    In particular, auto-regressive models look like this:\n",
    "\n",
    "    $X_t = \\beta_0 + \\Sigma^p_{i=1}\\beta_iX_{t-i} + \\epsilon_t$, <br/>\n",
    "    where $\\epsilon_t$ should be more or less accurately modeled by white noise.\n",
    "\n",
    "    We indicate how many terms our $AR$ model has by writing $AR(k)$ where $k$ is the number of terms.\n",
    "\n",
    "    Looking at the PACF can help us decide on an appropriate $p$: We can look at where the correlation values cross the confidence thresholds. <br/><br/>\n",
    "\n",
    "- 'MA' is for \"**Moving Average**\": The prediction for today will be a function of the rolling mean.\n",
    "\n",
    "    The number of average terms we want to include will be a parameter in the statsmodels model object (\"q\").\n",
    "\n",
    "    In  particular, moving-average models look like this:\n",
    "\n",
    "    $X_t = \\mu + \\epsilon_t + \\Sigma^q_{i=1}\\beta_i\\epsilon_{t-i}$, <br/>\n",
    "    where again the $\\epsilon$ should be modeled by white noise.\n",
    "\n",
    "    We indicate how many terms our $MA$ model has by writing $MA(k)$ where $k$ is the number of terms.\n",
    "\n",
    "    Looking at the ACF can help us decide on an appropriate $q$: We can look at where the correlation values cross the confidence thresholds.\n",
    "\n",
    "For some technical details, see [this page](https://en.wikipedia.org/wiki/Autoregressive%E2%80%93moving-average_model#Choosing_p_and_q)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $AR$ and $MA$ models are intimately related. In fact $AR(p)$ is equivalent to $MA(\\infty)$ for any $p$. The reverse holds as well if $|\\theta| < 1$ for all $\\theta$ in $MA(q)$. For more on this, see [here](https://otexts.com/fpp2/MA.html).\n",
    "\n",
    "Consider $AR(1)$:\n",
    "\n",
    "$X_t = \\beta_0 + \\beta_1X_{t-1} + \\epsilon_t$ <br/> \n",
    "$= \\beta_0 + \\beta_1(\\beta_1X_{t-2} + \\epsilon_{t-1})$ <br/>\n",
    "$= \\beta_0 + \\beta_1^2X_{t-2} + \\beta_1\\epsilon_{t-1}$ <br/>\n",
    "$= \\beta_0 + \\beta_1^3X_{t-3} + \\beta_1^2\\epsilon_{t-2} + \\beta_1\\epsilon_{t-1}$\n",
    "\n",
    "In the limit of this expansion we obtain an expression for $MA(\\infty)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stationarity and the Dickey-Fuller Test\n",
    "\n",
    "ARMA models assume that the time series is *stationary*, which means that its statistical properties are not a (meaningful) function of time.\n",
    "\n",
    "It may seem counterintuitive that, for modeling purposes, we want our time series not to be a function of time! But the basic idea is the familiar one that we want our datapoints to be mutually *independent*. For more on this topic, see [here](https://stats.stackexchange.com/questions/19715/why-does-a-time-series-have-to-be-stationary).\n",
    "\n",
    "One way of testing for stationarity is to use the Dickey-Fuller Test. The statsmodels version returns the test statistic and a p-value, relative to the null hypothesis that the series in question is NOT stationary. For more, see [this Wikipedia page](https://en.wikipedia.org/wiki/Augmented_Dickey%E2%80%93Fuller_test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Presumably, our football series is not stationary. Let's check.\n",
    "\n",
    "adfuller(fb['counts'], autolag=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But let's check the stationarity of the *differences* of our data. We'll use **`.diff()`**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb['counts'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb['counts'].diff().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adfuller(fb['counts'].diff()[1:], autolag=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 3\n",
    "q = 1\n",
    "\n",
    "# This model will have three auto-regressive terms and one moving-average term.\n",
    "\n",
    "ar = ARMA(fb['counts'].diff().values[1:], (p, q)).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_error(fb['counts'].diff().values[1:], ar.predict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(fb['counts'].diff().values[1:], ar.predict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `np.cumsum()`\n",
    "Let's use `np.cumsum()` to add up our predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb['counts'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cumsum(fb['counts']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = ar.predict()\n",
    "full = fb['counts'].values[0] +  np.cumsum(preds)\n",
    "\n",
    "f, a = plt.subplots(figsize=(20, 4))\n",
    "a.plot(fb.index[1:], fb['counts'][1:], 'r', label='Data')\n",
    "a.plot(fb.index[1:-1], full[1:], 'k', label='Fit')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, a = plt.subplots(figsize=(20, 4))\n",
    "a.plot(fb.index[1:], fb['counts'].diff()[1:], label='Data')\n",
    "a.plot(fb.index[1:-1], preds[1:], label='Fit')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unemployment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/seasonally-adjusted-quarterly-us.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = ['year_q', 'unemp_rate']\n",
    "data['unemp_rate'] = data['unemp_rate'].map(lambda x:\\\n",
    "                                            float(str(x).replace('%', '')))\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date'] = pd.to_datetime(data['year_q']).dt.to_period('Q')\n",
    "data.set_index('date', inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 2\n",
    "q = 0\n",
    "ar2 = ARMA(data['unemp_rate'].diff()[1:].values, (p, q)).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds2 = ar2.predict()\n",
    "full2 = data['unemp_rate'].values[0] + np.cumsum(preds2)\n",
    "\n",
    "f, a = plt.subplots(figsize=(20, 4))\n",
    "a.plot(data.index.to_timestamp()[1:], data['unemp_rate'][1:],\n",
    "       'r', label='Data')\n",
    "a.plot(data.index.to_timestamp()[1:], full2, 'k', label='Fit')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['unemp_rate'][-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# `.forecast()`: Let's check out the doc here.\n",
    "\n",
    "ar2.forecast(steps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['unemp_rate'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['unemp_rate'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['unemp_rate'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['unemp_rate'][-1] + np.cumsum(ar2.forecast(steps=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f, a = plt.subplots(figsize=(20, 4))\n",
    "a.plot(data.index.to_timestamp()[1:], data['unemp_rate'].diff()[1:],\n",
    "       label='Data')\n",
    "a.plot(data.index.to_timestamp()[1:], preds2, label='Fit')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARMA $\\rightarrow$ ARIMA $\\rightarrow$ SARIMA $\\rightarrow$ SARIMAX\n",
    "\n",
    "This idea of using the *differences* of our data points to make our data stationary and so appropriate for ARMA modeling is encoded in **ARIMA** modeling. The 'I' is for \"integrated\". **SARIMA** modeling adds a **S**easonal component. And **SARIMAX** modeling adds e**X**ogenous (independent) variables.\n",
    "\n",
    "Statsmodels supports these models as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see if we can do a better job with our football data by including a seasonal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_model = SARIMAX(fb['counts'], order=(3, 1, 1), freq='MS').fit()\n",
    "\n",
    "season_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb['counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_error(fb['counts'], season_model.predict()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this worse than before? Or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(fb['counts'], season_model.predict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, a = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "a.plot(fb.index, fb['counts'])\n",
    "a.plot(fb.index, season_model.predict());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
