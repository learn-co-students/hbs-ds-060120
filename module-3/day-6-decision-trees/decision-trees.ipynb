{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Decision Trees\n",
    "\n",
    "<img src=\"img/tree.jpeg\" width=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outcomes\n",
    "\n",
    "- summarize the intuitive logic behind decision trees\n",
    "- solve a small example with metric\n",
    "- explore the fine-tuning options in `sklearn` for decision trees\n",
    "- build a decision tree in `sklearn`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The key tool of decision trees is that some attributes provide more _information_ than others when trying to make a decision.<br>\n",
    "And we rank attributes in the hierarchy based on how useful they are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example 1 - when looking to monogamously date someone\n",
    "Which is more important?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Their taste in music?\n",
    "![music](img/music.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Or if they are married already?\n",
    "![married2](img/married2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example 2 - when looking for someone to pet sit your cat\n",
    "Which is more important?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How many cats **they** have:\n",
    "\n",
    "![cats](img/cats.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How many cats have **died** on their watch while pet sitting:\n",
    "\n",
    "![petcem](img/petcem.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example 3 with some vocab\n",
    "Rory is a teenager trying to decide if he wants to go to a party, and this is the decision tree represtinging his process:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### His decision tree\n",
    "<img src=\"img/party.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vocabulary: nodes\n",
    "\n",
    "<img src=\"img/terminology1.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vocabulary: splits & branches\n",
    "\n",
    "<img src=\"img/terminology2.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example 4: with data\n",
    "\n",
    "Suppose we're working on a classification algorithm designed to **sort customers into two classes: those who pay their sales bills and those who don't**.\n",
    "\n",
    "Each row in my dataframe represents a customer, and I have many predictors (columns) in my dataframe, including:\n",
    "\n",
    "- salary\n",
    "- total_bill\n",
    "- club_member (boolean)\n",
    "- years_post-sec_ed\n",
    "\n",
    "Let's look at a simple set of data. **The 'paid' column is our target or dependent variable.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "custs = pd.DataFrame([[45000, 1000, True, 2, False],\n",
    "                      [70000, 100, True, 10, True],\n",
    "                      [30000, 2000, False, 0, False],\n",
    "                      [90000, 500, True, 2, True],\n",
    "                      [70000, 200, True, 5, False]],\n",
    "                     columns=['salary', 'total_bill', 'club_member', 'years_post-sec_ed', 'paid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Partitioning\n",
    "\n",
    "I partition my data by asking a question about the independent variables. The goal is to ask the right questions in the right order so that the resultant groups are \"pure\" with respect to the dependent variable. More on this below!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's explore some variables below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "test = custs.sort_values(['salary'])\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Is the customer a club member?\n",
    "\n",
    "This would divide my data into two groups:\n",
    "\n",
    "- Group 1:\n",
    "\n",
    "data points: 0, 1, 3, 4 (dep. var.: False, True, True, False)\n",
    "\n",
    "- Group 2:\n",
    "\n",
    "data points: 2 (dep. var.: False)\n",
    "\n",
    "While I've isolated one of the customers who haven't paid in the second group, the first group is an even mix of payers and non-payers. So this split is not particularly good.\n",
    "\n",
    "Would a different question split our data more effectively? Let's try:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### \"Is the customer's salary less than $60k?\"\n",
    "\n",
    "This would divide my data into two groups:\n",
    "\n",
    "- Group 1:\n",
    "\n",
    "data points: 0, 2 (dep. var.: False, False)\n",
    "\n",
    "-  Group 2:\n",
    "\n",
    "data points: 1, 3, 4 (dep. var.: True, True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Which feature is more accurate in predicting whether and individual pays their bill?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Entropy and Information Gain\n",
    "\n",
    "The goal is to have our ultimate classes be fully \"ordered\" (for a binary dependent variable, we'd have the 1's in one group and the 0's in the other). So one way to assess the value of a split is to measure how *disordered* our groups are, and there is a notion of *entropy* that measures precisely this.\n",
    "\n",
    "The entropy of the whole dataset is given by:\n",
    "\n",
    "$\\large E = -\\Sigma_i p_i\\log_2(p_i)$,\n",
    "\n",
    "where $p_i$ is the probability of belonging to the $i$th group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Entropy will always be between 0 and 1. The closer to 1, the more disordered your group.**\n",
    "\n",
    "To repeat, in the present case we have only two groups of interest: the payers (2/5) and the non-payers (3/5).\n",
    "\n",
    "So our entropy for this toy dataset is:\n",
    "\n",
    "$-0.4*\\log_2(0.4) -0.6*\\log_2(0.6)$.\n",
    "\n",
    "Let's use the ```math``` library to calculate this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To calculate the entropy of a *split*, we're going to want to calculate the entropy of each of the groups made by the split, and then calculate a weighted average of those groups' entropies––weighted, that is, by the size of the groups. Let's calculate the entropy of the split produced by our question above about salary:\n",
    "\n",
    "Group 1:\n",
    "\n",
    "$E_{g1} = 0 * \\log_2(0) - 1 * \\log_2(1) = 0$. This is a pure group! The probability of being a payer in Group 1 is 0 and the probability of being a non-payer in Group 1 is 1.\n",
    "\n",
    "Group 2:\n",
    "\n",
    "$E_{g2} = \\frac{2}{3} * \\log_2\\left(\\frac{2}{3}\\right) - \\frac{1}{3} * \\log_2\\left(\\frac{1}{3}\\right)$.\n",
    "\n",
    "Once again, using ```math```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To get the whole entropy for this split, we'll do a weighted sum of the two group entropies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For a given split, the **information gain** is simply the entropy of the parent group less the entropy of the split.\n",
    "\n",
    "For a given parent, then, we maximize our model's performance by *minimizing* the split's entropy.\n",
    "\n",
    "What we'd like to do then is:\n",
    "\n",
    "1. to look at the entropies of all possible splits, and\n",
    "2. to choose the split with the lowest entropy.\n",
    "\n",
    "In practice there are far too many splits for it to be practical for a person to calculate all these different entropies ...\n",
    "\n",
    "... but we can make computers do these calculations for us!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gini Impurity\n",
    "\n",
    "An alternative metric to entropy comes from the work of Corrado Gini. The Gini Impurity is defined as:\n",
    "\n",
    "$\\large G = 1 - \\Sigma_i p_i^2$,\n",
    "\n",
    "where, again, $p_i$ is the probability of belonging to the $i$th group.\n",
    "\n",
    "**Gini Impurity will always be between 0 and 0.5. The closer to 0.5, the more disordered your group.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Exercise: Calculate the Gini Impurity for our toy dataset above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Coding Trees in Python\n",
    "\n",
    "Scikit-learn has a tree module, which houses both a DecisionTreeClassifier and a DecisionTreeRegressor. The difference, as is probably clear by now, is that the former is for classification problems (discrete target) and the latter is for regression problems (continuous target). Let's use the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Trees in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "# tree.plot_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree.export_graphviz()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluating a decision tree:\n",
    "- Train and test - use predict to compare auc of test and train\n",
    "- \"prune\" the tree: adjust minimum number of samples required at a leaf node or a split as well as setting the maximum depth of the tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pros and Cons of Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Pros:\n",
    "- Easy to Understand\n",
    "- Useful in Data exploration\n",
    "- Less data cleaning required\n",
    "- Data type is not a constraint\n",
    "- Non Parametric Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Cons:\n",
    "- Prone to over-fitting\n",
    "- Struggles with creating cut-off splits with continuous variables\n",
    "- Non Parametric Method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breast Cancer identification dataset\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/breast_cancer.csv', index_col=0)\n",
    "df = df.drop(columns=['Unnamed: 32'])\n",
    "df['Target'] = df.diagnosis.map(lambda x: 1 if x == 'M' else 0)\n",
    "df = df.drop(columns='diagnosis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outcome Review\n",
    "\n",
    "- summarize the intuitive logic behind decision trees\n",
    "- solve a small example with metric\n",
    "- explore the fine-tuning options in `sklearn` for decision trees\n",
    "- build a decision tree in `sklearn`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
