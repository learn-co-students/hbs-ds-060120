{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Metrics and Logistic Regression Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start off with a page from [Google's Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/classification/true-false-positive-negative) and talk about a classic classification problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Boy Who Cried 'Wolf'\n",
    "\n",
    "![adorable wolf image from instagram user fablefire: https://www.instagram.com/p/CCGgVLGFneE/](images/awoo.png)\n",
    "\n",
    "In the old fable about the boy who cried 'wolf' there are two possible outcomes: \n",
    "\n",
    "- **No Wolf** - negative outcome, or 0\n",
    "- **Wolf** - positive outcome, or 1\n",
    "\n",
    "(I know, having a wolf arrive is not \"positive\" - but it is what we're trying to predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you think of this as a model, where the shepherd is predicting whether or not a wolf will threaten the flock of sheep:\n",
    "\n",
    "![outcome description for wolf scenarios as a confusion matrix](images/wolf_confusion_matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what does that look like with data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All of the imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix # New to version 0.22.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/titanic.csv', index_col = \"PassengerId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-less Baseline\n",
    "\n",
    "First of all, I want to see how well the model will do if it predicts the majority class. In other words, if the model only predicts that no one survives, what percentage of the time would it be right? \n",
    "\n",
    "How do we do this? Find the number of passengers who didn't survive, divide by the total number of passengers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_passengers = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_passenger_didnotsurvive = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find percent who did not survive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Another way to do this - the mean of this column gives the percentage of\n",
    "# passengers who DID survive because it's a 0/1 binary - find the inverse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = df['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_baseline = [0] * num_passengers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A confusion matrix\n",
    "confusion_matrix(y_actual, y_pred_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, prettier: \n",
    "\n",
    "<img alt=\"table view with colors to show results of modelless baseline\" src=\"images/titanic_modelless_baseline_cm.png\" height=200 width=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix &rarr; Classification Metrics\n",
    "\n",
    "That block above, where we hashed out true negatives / true positives / false negatives / false positives, is called a **Confusion Matrix** - a summary of how well a classification model was able to predict each class. Across one axis you have the _predicted_ labels, and across the other axis you have the _actual_ labels, and thus you're able to clearly see the breakdown of where a model is making mistakes - and, more importantly, what kinds of mistakes your model is making.\n",
    "\n",
    "So - how does a confusion matrix translate into classification metrics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix Interpretation\n",
    "\n",
    "\n",
    "<img alt=\"confusion matrix interpretation with metrics\" src=\"images/confusion_matrix_interpretation.png\" height=600 width=600>\n",
    "\n",
    "Note that I've highlighted the most often used metrics in blue above. \n",
    "\n",
    "In other words, those metrics are:\n",
    "\n",
    "- Accuracy: All True Predictions / All Predictions\n",
    "\n",
    "- Precision score: TP / All Predicted Positives\n",
    "\n",
    "- Recall or Sensitivity: TP / All Actual Positives \n",
    "\n",
    "There's one more score that's often referenced which balances precision and recall - it's called an [**F1 Score**](https://en.wikipedia.org/wiki/F1_score).\n",
    "\n",
    "$$ \\text{F1 Score} = 2 * \\frac{ precision * recall}{precision + recall} $$\n",
    "\n",
    "\n",
    "**Let's Discuss**: Why might we care more about precision than recall, or vice versa?\n",
    "\n",
    "- \n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "Moving on - luckily SKLearn will of course calculate these scores for us. You can see all of their classification metrics [here](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "![monty python, 'nobody expects the logistic regression](images/nobodyexpectslogreg.jpg)\n",
    "\n",
    "In general, I'll be following the imputation/scaling strategy outlined [here](https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html) (just in a more laid-out way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to use the columns: `Age`, `Fare`, `Sex`, and `Pclass` - a combination of types, also `Age` has null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's encode our categorical column, Sex\n",
    "\n",
    "enc_sex = pd.get_dummies(df['Sex'], drop_first=True)\n",
    "\n",
    "# Add the column\n",
    "df['isMale'] = enc_sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining my X and y\n",
    "\n",
    "X = df[['Pclass', 'isMale', 'Age', 'Fare']]\n",
    "y = df['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And then doing a train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Age, I need to impute nulls\n",
    "age_imputer = SimpleImputer(strategy='median')\n",
    "X_train_nonulls = age_imputer.fit_transform(X_train)\n",
    "X_test_nonulls = age_imputer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's scale our data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train_nonulls)\n",
    "X_test_scaled = scaler.transform(X_test_nonulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate our model\n",
    "logreg = LogisticRegression(class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, fitting our model and grabbing our training and testing predictions\n",
    "logreg.fit(X_train_nonulls, y_train)\n",
    "\n",
    "train_preds = logreg.predict(X_train_scaled)\n",
    "test_preds = logreg.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the confusion matrix using SKLearn\n",
    "plot_confusion_matrix(logreg, X_test_scaled, y_test,\n",
    "                      cmap=plt.cm.Blues, # Changing the color scheme\n",
    "                      values_format=\".3g\") # Formatting the numbers properly\n",
    "plt.grid(False) # This just removes an annoying grid that shows up by default\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Printing the metrics nicely\n",
    "metrics = {\"Accuracy\": accuracy_score,\n",
    "           \"Recall\": recall_score,\n",
    "           \"Precision\": precision_score,\n",
    "           \"F1-Score\": f1_score}\n",
    "\n",
    "for name, metric in metrics.items():\n",
    "    print(f\"{name}:\"); print(\"=\"*len(name))\n",
    "    print(f\"TRAIN: {metric(y_train, y_train_pred):.4f}\")\n",
    "    print(f\"TEST: {metric(y_test, y_test_pred):.4f}\")\n",
    "    print(\"*\" * 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation:\n",
    "\n",
    "So, how'd we do? Specifically, how'd we do compared to our model-less baseline?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "What can we potentially do to improve this model?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neat Tricks\n",
    "\n",
    "Let's try the code directly from the link above, which uses some neat tricks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['Age', 'Fare']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_features = ['Sex', 'Pclass']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression())])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns='Survived'), y, test_size=0.2)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the confusion matrix using SKLearn\n",
    "plot_confusion_matrix(clf, X_test, y_test,\n",
    "                      cmap=plt.cm.Blues, # Changing the color scheme\n",
    "                      values_format=\".3g\") # Formatting the numbers properly\n",
    "plt.grid(False) # This just removes an annoying grid that shows up by default\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, metric in metrics.items():\n",
    "    print(f\"{name}:\"); print(\"=\"*len(name))\n",
    "    print(f\"TRAIN: {metric(y_train, y_train_pred):.4f}\")\n",
    "    print(f\"TEST: {metric(y_test, y_test_pred):.4f}\")\n",
    "    print(\"*\" * 15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
